<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <title>SatisfIA</title>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wdth,wght@0,62.5..100,300..700;1,62.5..100,300..700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="assets/css/style.css">
        <link rel="apple-touch-icon" sizes="180x180" href="assets/img/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="assets/img/favicon-16x16.png">
        <link rel="mask-icon" href="assets/img/safari-pinned-tab.svg" color="#5bbad5">
        <meta name="msapplication-TileColor" content="#da532c">
        <meta name="theme-color" content="#ffffff">
        <link rel="manifest" href="site.webmanifest">
    </head>
    <body>
        <div class="container">
            <img src="assets/img/logo_colored.svg" width="100%">
            <h2>The SatisfIA project</h2>
            <p>
                We are an interdisciplinary research team developing aspiration-based designs for intelligent agents. 
		The project is hosted by the <a href="https://www.pik-potsdam.de/en/institute/futurelabs/gane/futurelab-gane">FutureLab on Game Theory &amp; Networks of Interacting Agents</a> at PIK 
		    in collaboration with the <a href="https://aisafety.camp">AI Safety Camp</a>, 
		    <a href="https://sparai.notion.site/Supervised-Program-for-Alignment-Research-SPAR-4da6be132e974823961abfdd0c218536">SPAR</a>, 
		    and <a href="https://www.ens.psl.eu/en">ENS Paris</a>, led by <a href="https://www.pik-potsdam.de/members/heitzig">Jobst Heitzig</a>.
		    We are currently looking for funding and are open for further collaborations.
	    </p>
	    <p>
                Our original contribution to AI safety research is our focus on <strong>non-maximizing agents</strong>. 
		The project’s approach diverges from traditional AI designs that are based on the idea of maximizing objective functions, 
		which is unsafe if these objective functions are not perfectly aligned with actually desired outcomes.
	    </p>
	    <p>
                Instead, SatisfIA’s AI agents are designed to fulfill goals specified through constraints known as <strong>aspirations</strong>, 
		reducing the likelihood of extreme actions and increasing safety.
            </p>
		<p>
			This is part of a broader agenda of designing agents in safer ways that you can learn about 
			in <a href="https://pik-potsdam.zoom-x.de/rec/share/nl-EAnoEGGxqvwSZvh12tovUkM784Hlo7ogDezTWCA1rvuUMUDunLdAXsp8Qy4-k.QbpcNkpL1V_aaxw_">this talk at the ENS Paris</a>.
			There's also an earlier <a href="https://www.youtube.com/watch?v=zX0qq0K5z9c">interview on Will Petillo's Y*uTube channel</a> 
			where Jobst talks about the rationale of non-maximizing 
			(and also about “satisficing”, an alternative but related idea to tihis project's, see below).   
		</p>
		
            <h2>Research focus</h2>
            <p>
                Our research explores various designs for non-maximizing agents that operate in complex environments under several forms on uncertainty.
		These environments are modelled as fully or partially observed <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Processes</a>.
		We also develop corresponding planning and/or learning algorithms, 
		primarily variations of model-based planning and <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>.
	    </p>
	    <p>
		This involves the theoretical design of agents and algorithms, implementing them in software (mostly using Python and/or <a href="https://webppl.org/">WebPPL</a>), simulating their behavior in test environments such as <a href="https://github.com/google-deepmind/ai-safety-gridworlds">AI safety gridworlds</a>, and analyzing their behavior and safety implications. The goal is to provide numerical evidence and formal proofs where possible, and to contribute to the academic and non-academic discourse through publications and <a href="https://www.lesswrong.com/s/4TT69Yt5FDWijAWab">explanatory blog posts</a>.
            </p>
            <h2>Motivation and background</h2>
            <p>
                The motivation for this research stems from a growing consensus among AI researchers and the public about the potential existential threat posed by powerful AI agents if not properly aligned with human values and safety concerns.<br>
                In the worst case scenario it won’t be possible to align AIs with human values, since 
                <ul>
                    <li>there might not be a set of universally acceptable values, </li>
                    <li>they might not be formalizable,</li>
                    <li>if they are, it might not be possible to discover or implement them before risk-factors start appearing.</li>
                </ul>
                By adopting a non-maximizing, aspiration-based approach, we aim to address these safety concerns directly by helping designing AI agents that operate in a safer and maybe also more predictable way.
            </p>
            <h2>Theoretical foundations</h2>
            <h3>Aspiration-based goals</h3>
            <p>
                In contrast to traditional AI, we do <em>not</em> think that one should give a powerful AI system the goal or task to take that action which results in the largest possible value of some objective function (“maximizing”). Instead, we assume that goals and tasks for powerful AI systems should be formulated <em>without any reference to such a function at all</em>. Instead, goals should be specified by requesting that certain observables or variables should fall into certain desirable ranges. 
	    </p>
	    <p>
		E.g., a powerful climate-managing AI system should rather be given a task such as “keep global mean temperature in the range between zero and two degrees warming without decreasing global GDP”, rather than “maximize this or that complicated function of temperature and GDP”.</p>
            <p>
		These desirability ranges are called <strong>aspirations</strong> in our project. Since aspiration-based goals are typically easier to fulfill than maximizing a certain objective function, the agent gains some freedom in choosing <em>how</em> exactly to achieve them. Hence we can design agents so that they use this freedom to fulfill the goals in ways that are generally <em>safer</em> than what a maximizing agent would do.
	    </p>
	    <p>
		In order to find these safer ways of satisfying the aspirations, our designs make use of a number of <strong>safety criteria</strong> such as
                <ul>
                    <li>how much the agent changes or disturbs its environment,</li>
                    <li>how predictable or conventional its actions are,</li>
                    <li>how much influence or resources it acquires or uses.</li>
                </ul>
	    </p>
                <details>
                    <summary>
                    This approach is fundamentally different from the alternative approach of <a href="https://en.wikipedia.org/wiki/Satisficing">“satisficing”</a>
                    </summary><div>
            <p>
                 In the satisficing approach, the agent <em>does</em> have an objective function, which has the general interpretation of “more is better”, but which might still not reflect <em>all</em> aspects of the agent’s preferences. In such a case, bringing about the maximum of that imperfect objective function would incur too large costs of a type not included in the specification of the objective function, and would thus not be optimal or even advisable.<br>
                A satisficing agent would thus choose to <em>not go all the way</em> towards the maximum of the imperfect objective function, but would rather stop searching for further increases in the objective function once a certain value of that function has been reached.
            </p>
            </div>
                </details>
            <h3>Goodhart’s law</h3>
            <p>
                Even though behavioral science suggests that <em>human</em> behavior can sometimes be well modeled as “satisficing” behavior, we do <em>not</em> adopt a “satisficing” approach here because it does not address <em>another</em> common safety risk from using objective functions: “Goodhart’s law.”
            </p>
            <p>
                <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s law</a> serves as a critical caution in the formulation and pursuit of goals, especially in complex systems such as those a general-purpose AI system will typically find itself in. The law consists in the finding that, usually,
<blockquote>
	<p>when a measure becomes a target, it ceases to be a good measure.</p>
</blockquote>
This principle highlights a fundamental risk in setting objectives, particularly in AI-driven endeavors. By making a specific measure the objective function of an AI agent’s actions, we inadvertently shift its goals. The agent, in striving to maximize that objective function, may exploit loopholes or take shortcuts that <em>align with the letter of the goal but deviate from its intended spirit</em>. This will usually lead to unintended consequences and side-effects, where the pursuit of a narrowly defined objective overshadows broader, not explicitly specified considerations, such as ethical implications, societal impact, or long-term sustainability.
            </p>
            <p>
	            <img src="assets/img/dont_maximize.png" width="30%" style="float: right;">
                In the context of our research, Goodhart’s law underscores the importance of designing AI agents whose goals are <em>not</em> the full or partial maximization of some objective function. Instead, by embracing aspiration-based designs, we aim to create systems that are inherently more aligned with holistic and adaptable criteria.<br>
                This approach seeks to mitigate the risks associated with Goodhart’s law by ensuring that the metrics used to guide AI behavior are not fixed targets but rather flexible aspirations that encourage the agent to consider a wider range of outcomes and behaviors. Thus, our project recognizes and addresses the challenge posed by Goodhart’s law, advocating for a more nuanced and safety-conscious strategy in the development of AI systems.
            </p>
            <h2>Challenges</h2>
            <ul>
                <li aria-level="1">
                    Dealing with different forms of uncertainty, incoming data, changing circumstances or goals, chance, and good or bad luck.
                </li>
                <li aria-level="1">
                    Adapting existing model-based planning and reinforcement learning algorithms from the maximizing context into the aspiration-based context.
                </li>
                <li aria-level="1">
                    Dealing with goals specified as constraints on several variables, such as temperature and precipitation, without merging them into one joint indicator.
                </li>
            </ul>
            <h2>Contributions and Impact</h2>
            <p>
                By advancing the understanding and implementation of non-maximizing, aspiration-based AI agents, the SatisfIA project aims to contribute significantly to the field of AI safety. The project’s outcomes will include well-documented software components, academic papers, and educational materials to disseminate our findings and encourage further research in this vital area.
            </p>
            <p>
                In essence, SatisfIA is pioneering a shift towards safer AI by embedding the principles of aspiration-based decision making at the core of AI agent design. It can be seen as a form of AI that is <a href="https://groups.google.com/g/safe-by-design/about">“safe-by-design”</a>. This research not only challenges conventional maximization paradigms but also opens new avenues for creating AI systems that are not contradicting human values and safety requirements. Not the least, this research poses interesting mathematical and engineering questions!
            </p>
            <h2>The Team</h2>
		<p>
			Our team currently consists of about 15 volunteers with various backgrounds, organized into small, 
			partially overlapping sub-teams of two-to-three persons, working part-time on:
			<ul>
				<li>General theory of safe, goal-oriented behavior</li>
				<li>Theory of information-theoretic safety criteria</li>
				<li>Design and implementation of model-based planning algorithms</li>
				<li>Design, implementation, and curation of safety-relevant test environments</li>
				<li>Documentation, quality control, and educational material</li>
				<li>Design and implementation of tabular and deep learning algorithms</li>
				<li>Theory of multi-dimensional aspirations</li>
				<li>Theory of aspiration-based hierarchical reinforcement learning</li>
				<li>Theory of aspiration-based behavior in multi-agent environments</li>
			</ul>
		</p>
		<p>Here are some of our team members:</p>
            <div class="team">
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/ak.jpg">
                    </div>
                    <div class="name">Ariel Kwiatkowski</div>
                    <div class="description">AI Researcher</div>
                    <div class="tags">
                        <div class="tag">Deep Reinforcement Learning</div>
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/bj.jpg">
                    </div>
                    <div class="name">
                    Bob Jacobs 
                    </div>
                    <div class="description">
                    Student of moral science at the University of Ghent
                    </div>
                    <div class="tags"><div class="tag">Overall theory</div>
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/jh.png">
                    </div>
                    <div class="name">
                    Jobst Heitzig
                    </div>
                    <div class="description">
                    Project lead
                    </div>
                    <div class="tags">
                    <div class="tag">Project management</div>
                    <div class="tag">Overall theory</div>
                    <div class="tag">Adjusting goals over time</div>
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/jo.jpg">
                    </div>
                    <div class="name">Joss Oliver</div>
                    <div class="description">AI Safety Researcher</div>
                    <div class="tags">
                        <div class="tag">Reinforcement Learning</div>
                        <div class="tag">Theory</div>
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/mk.jpg">
                    </div>
                    <div class="name">
                    Martin Kunev
                    </div>
                    <div class="description">
                    AI engineer
                    </div>
                    <div class="tags">
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/mr.jpg">
                    </div>
                    <div class="name">
                        Massimo Redaelli
                    </div>
                    <div class="description">
                        Software Engineer
                    </div>
                    <div class="tags">
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/sj.jpg">
                    </div>
                    <div class="name">
                        Sai Joseph
                    </div>
                    <div class="description">
                        Master’s student in Computer science, Northeastern University
                    </div>
                    <div class="tags">
                        <div class="tag">Deep Reinforcement Learning</div>
                    </div>
                </div>
                <div class="member">
                    <div class="name">
                        Simon Dima 
                    </div>
                    <div class="description">
                        Master’s student in Computer science, <span lang="fr">École Normale Supérieure</span>
                    </div>
                    <div class="tags"> </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/sf.jpg">
                    </div>
                    <div class="name">
                    Simon Fischer
                    </div>
                    <div class="description">
                    Independent AI safety researcher
                    </div>
                    <div class="tags">
			    <div class="tag">Overall theory</div>  
			    <div class="tag">Information theory</div>
                    </div>
                </div>
		<div class="member">
                    <div class="portrait">
                        <img src="assets/img/sr.png">
                    </div>
                    <div class="name">
                    Simon Rubinstein-Salzedo
                    </div>
                    <div class="description">
                    Mathematician
                    </div>
                    <div class="tags">
			    <div class="tag">AI safety criteria</div>  
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/vi.jpg">
                    </div>
                    <div class="name">
                    Vladimir Ivanov
                    </div>
                    <div class="description">
                    Master’s CS student
                    </div>
                    <div class="tags">
                        <div class="tag">Deep learning</div>
                    </div>
                </div>
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/wp.jpg">
                    </div>
                    <div class="name">Will Petillo</div>
                    <div class="description">Videogame developer</div>
                    <div class="tags">
                        <div class="tag">Gridworlds</div>
                    </div>
                </div>
                <!--
                <div class="member">
                    <div class="portrait">
                        <img src="assets/img/.jpg">
                    </div>
                    <div class="name">
                    </div>
                    <div class="description">
                    </div>
                    <div class="tags">
                        <div class="tag"></div>
                    </div>
                </div>
                -->
            </div>
            <h2>Contact</h2>
            <p>You can reach us via <a href="mailto:heitzig@pik-potsdam.de">email to Jobst</a>.
	    </p>
        </div>
    </body>
</html>

