#!/usr/bin/env python3
"""
Coordinated IQL Algorithm optimized for door-key coordination scenarios.
"""

import numpy as np
import os
from datetime import datetime
from fixed_enhanced_algorithm import FixedEnhancedTwoPhaseTimescaleIQL

class CoordinatedTwoPhaseTimescaleIQL(FixedEnhancedTwoPhaseTimescaleIQL):
    """Enhanced version with better coordination for door-key scenarios"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Enhanced parameters for coordination
        self.coordination_bonus = 5.0  # Bonus for enabling human progress
        self.door_open_bonus = 10.0    # Large bonus for opening doors
    
    def _is_door_blocking_human_path(self, env, human_id, goal):
        """Check if there's a door blocking the human's direct path to goal"""
        human_pos = env.agent_positions[human_id]
        
        # Check if direct path to goal is blocked by doors
        # For simple_map4: human at (1,3), goal at (3,3), door at (2,3)
        if human_pos == (1, 3) and tuple(goal) == (3, 3):
            # Check if door at (2,3) is closed
            if hasattr(env, 'grid') and hasattr(env.grid, 'get'):
                cell = env.grid.get(2, 3)
                if cell and hasattr(cell, 'type') and 'door' in cell.type.lower():
                    return not cell.is_open  # Door is blocking if closed
        
        return False
    
    def _get_door_positions(self, env):
        """Get positions of all doors in the environment"""
        doors = []
        if hasattr(env, 'grid'):
            for x in range(env.grid.width if hasattr(env.grid, 'width') else 5):
                for y in range(env.grid.height if hasattr(env.grid, 'height') else 5):
                    if hasattr(env.grid, 'get'):
                        cell = env.grid.get(x, y)
                        if cell and hasattr(cell, 'type') and 'door' in cell.type.lower():
                            doors.append((x, y))
        return doors
    
    def _enhanced_power_reward(self, env, actions, prev_positions):
        """Enhanced power reward with coordination bonuses"""
        power_reward = 0.0
        goal_reached = False
        
        human_id = self.human_agent_ids[0]
        robot_id = self.robot_agent_ids[0]
        goal = env.human_goals[human_id]
        
        current_human_pos = tuple(env.agent_positions[human_id])
        prev_human_pos = prev_positions[human_id]
        
        # Base goal reward
        if current_human_pos == tuple(goal):
            power_reward += 10.0
            goal_reached = True
        
        # Coordination rewards for robot helping human
        # 1. Bonus for robot actions that enable human progress
        robot_action = actions.get(robot_id, 0)
        robot_pos = tuple(env.agent_positions[robot_id])
        
        # Large bonus for robot opening doors (toggle action near doors)
        if robot_action == 5:  # toggle action
            doors = self._get_door_positions(env)
            for door_pos in doors:
                # Check if robot is adjacent to door
                if (abs(robot_pos[0] - door_pos[0]) + abs(robot_pos[1] - door_pos[1])) == 1:
                    power_reward += self.door_open_bonus
                    print(f"    üîì Robot opened door at {door_pos}! (+{self.door_open_bonus})")
        
        # Bonus for robot collecting keys near doors that block human path
        if robot_action == 3:  # pickup action
            # Special bonus for picking up keys that can help human
            power_reward += 2.0
        
        # 2. Progress-based reward for human movement toward goal
        prev_dist = abs(prev_human_pos[0] - goal[0]) + abs(prev_human_pos[1] - goal[1])
        curr_dist = abs(current_human_pos[0] - goal[0]) + abs(current_human_pos[1] - goal[1])
        
        if curr_dist < prev_dist:
            progress_reward = 2.0 if curr_dist <= 1 else 1.0  # Extra bonus when close
            power_reward += progress_reward
        elif curr_dist > prev_dist:
            power_reward -= 0.5  # Small penalty for moving away
        
        # 3. Coordination bonus: reward robot for being helpful when human is stuck
        if prev_human_pos == current_human_pos and not goal_reached:
            # Human didn't move - robot should be doing something useful
            if robot_action in [3, 5]:  # pickup or toggle
                power_reward += self.coordination_bonus
        
        return power_reward, goal_reached
    
    def train_phase2_coordinated(self, env, episodes, max_steps=150):
        """Enhanced Phase 2 with coordination-focused training"""
        print("Starting Coordinated Phase 2: Learning robot-human coordination.")
        
        successful_episodes = 0
        
        for ep in range(episodes):
            env.reset()
            goal = env.human_goals[self.human_agent_ids[0]]
            episode_power_reward = 0.0
            episode_success = False

            for step in range(max_steps):
                actions = {}
                
                # Store previous positions
                prev_positions = {}
                for hid in self.human_agent_ids:
                    prev_positions[hid] = tuple(env.agent_positions[hid])
                
                # Robot action with enhanced exploration for coordination
                for rid in self.robot_agent_ids:
                    state_r = self.get_full_state(env, rid)
                    # Higher exploration during early training for coordination
                    exploration = 0.2 if ep < episodes * 0.3 else 0.1
                    actions[rid] = self.sample_robot_action_phase2(rid, state_r, env, epsilon=exploration)

                # Human action
                for hid in self.human_agent_ids:
                    state_h = self.get_human_state(env, hid, self.state_to_tuple(goal))
                    # Slightly higher exploration to find coordination patterns
                    actions[hid] = self.sample_human_action_effective(hid, state_h, self.state_to_tuple(goal), epsilon=0.15)

                next_obs, rewards, terms, truncs, _ = env.step(actions)
                done = any(terms.values()) or any(truncs.values())

                # Enhanced power reward calculation with coordination
                power_reward, goal_reached = self._enhanced_power_reward(env, actions, prev_positions)
                episode_power_reward += power_reward
                
                if goal_reached:
                    episode_success = True

                # Update Q_r with enhanced power rewards
                for rid in self.robot_agent_ids:
                    state_r = self.get_full_state(env, rid)
                    next_state_r = self.get_full_state(env, rid)
                    action_r = actions[rid]

                    if self.network:
                        next_q_values = self.q_r_backend.get_q_values(rid, next_state_r)
                        target = power_reward + self.gamma_r * np.max(next_q_values)
                        self.q_r_backend.update_q_values(rid, state_r, action_r, target, self.alpha_r)
                    else:
                        if state_r not in self.q_r[rid]:
                            self.q_r[rid][state_r] = np.zeros(len(self.action_space_dict[rid]))
                        if next_state_r not in self.q_r[rid]:
                            self.q_r[rid][next_state_r] = np.zeros(len(self.action_space_dict[rid]))
                            
                        old_q_r = self.q_r[rid][state_r][action_r]
                        next_max_q_r = np.max(self.q_r[rid][next_state_r])
                        new_q_r = old_q_r + self.alpha_r * (power_reward + self.gamma_r * next_max_q_r - old_q_r)
                        self.q_r[rid][state_r][action_r] = new_q_r

                # Update Q^e_h with enhanced shaping
                for hid in self.human_agent_ids:
                    goal_tuple = self.state_to_tuple(goal)
                    state_h = self.get_human_state(env, hid, goal_tuple)
                    next_state_h = self.get_human_state(env, hid, goal_tuple)
                    action_h = actions[hid]
                    
                    true_reward = 1.0 if tuple(env.agent_positions[hid]) == goal_tuple else 0.0
                    shaped_reward = self._shaped_reward(
                        prev_positions[hid], env.agent_positions[hid], goal_tuple, true_reward
                    )

                    if self.network:
                        next_q_values = self.q_e_backend.get_q_values(hid, next_state_h[:-2], goal_tuple)
                        target = shaped_reward + self.gamma_h * np.max(next_q_values)
                        self.q_e_backend.update_q_values(hid, state_h[:-2], action_h, target, self.alpha_e, goal_tuple)
                    else:
                        if state_h not in self.q_e[hid]:
                            self.q_e[hid][state_h] = np.zeros(len(self.action_space_dict[hid]))
                        if next_state_h not in self.q_e[hid]:
                            self.q_e[hid][next_state_h] = np.zeros(len(self.action_space_dict[hid]))
                            
                        old_q_e = self.q_e[hid][state_h][action_h]
                        next_max_q_e = np.max(self.q_e[hid][next_state_h])
                        new_q_e = old_q_e + self.alpha_e * (shaped_reward + self.gamma_h * next_max_q_e - old_q_e)
                        self.q_e[hid][state_h][action_h] = new_q_e

                if done or goal_reached:
                    if goal_reached:
                        successful_episodes += 1
                        print(f"    üéâ Goal reached in episode {ep+1}! (Power: {episode_power_reward:.1f})")
                    break
                    
            if (ep + 1) % 100 == 0:
                success_rate = successful_episodes / (ep + 1)
                print(f"  Phase 2, Episode {ep+1}/{episodes}. Success rate: {success_rate:.1%}")

def train_coordinated_algorithm(map_name, phase1_episodes=600, phase2_episodes=1000):
    """Train the coordinated algorithm for 100% success"""
    print(f"=== Coordinated Training on {map_name} ===")
    
    # Import map functions
    from envs.simple_map import get_map as get_simple_map
    from envs.simple_map2 import get_map as get_simple_map2
    from envs.simple_map3 import get_map as get_simple_map3
    from envs.simple_map4 import get_map as get_simple_map4
    from env import CustomEnvironment
    
    map_functions = {
        "simple_map": get_simple_map,
        "simple_map2": get_simple_map2,
        "simple_map3": get_simple_map3,
        "simple_map4": get_simple_map4,
    }
    
    get_map_func = map_functions[map_name]
    map_layout, map_metadata = get_map_func()
    
    env = CustomEnvironment(
        grid_layout=map_layout,
        grid_metadata=map_metadata,
        render_mode=None
    )
    
    print(f"Map: {map_metadata['name']}")
    print(f"Goal: {map_metadata['human_goals']}")
    
    robot_agent_ids = ["robot_0"]
    human_agent_ids = ["human_0"]
    action_space_dict = {
        "robot_0": list(range(6)),
        "human_0": list(range(3))
    }
    
    human_goals = list(map_metadata["human_goals"].values())
    G = [np.array(goal) for goal in human_goals]
    mu_g = [1.0 / len(G)] * len(G)
    
    # Create coordinated algorithm
    iql = CoordinatedTwoPhaseTimescaleIQL(
        alpha_m=0.5,  # Higher learning rates for faster coordination learning
        alpha_e=0.5,
        alpha_r=0.5,
        alpha_p=0.5,
        gamma_h=0.95,
        gamma_r=0.95,
        beta_r_0=6.0,  # Higher temperature for better exploration
        G=G,
        mu_g=mu_g,
        action_space_dict=action_space_dict,
        robot_agent_ids=robot_agent_ids,
        human_agent_ids=human_agent_ids,
        network=False,
        env=env
    )
    
    # Training with coordination focus
    print(f"\n--- Phase 1 Training ({phase1_episodes} episodes) ---")
    iql.train_phase1_fixed(env, episodes=phase1_episodes, max_steps=100)
    
    print(f"\n--- Coordinated Phase 2 Training ({phase2_episodes} episodes) ---")
    iql.train_phase2_coordinated(env, episodes=phase2_episodes, max_steps=150)
    
    # Test the policy extensively
    print(f"\n--- Testing Coordinated Policy (50 episodes) ---")
    success_rate, avg_steps = iql.test_policy(env, episodes=50, max_steps=200, verbose=True)
    
    # Save if successful
    if success_rate >= 0.8:
        os.makedirs("coordinated_checkpoints", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_path = os.path.join("coordinated_checkpoints", f"{map_name}_coordinated_{timestamp}.pkl")
        iql.save_models(checkpoint_path)
        print(f"Coordinated model saved: {checkpoint_path}")
    else:
        checkpoint_path = None
    
    return checkpoint_path, success_rate, avg_steps

if __name__ == "__main__":
    # Focus on simple_map4
    checkpoint, success_rate, avg_steps = train_coordinated_algorithm("simple_map4", 800, 1200)
    print(f"\nFINAL COORDINATED RESULT: {success_rate:.1%} success, {avg_steps:.1f} avg steps")
    
    if success_rate >= 0.95:
        print("üéâ SUCCESS! Coordinated algorithm achieves ‚â•95% success!")
    else:
        print("‚ö†Ô∏è Still needs more coordination tuning")