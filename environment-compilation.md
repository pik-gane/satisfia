# Tierlist of environments to test SatisfIA on

This is a pruned version of [this list](https://docs.google.com/spreadsheets/d/1uXzWavy1mS0X-uQ21UPWHlAHjXFJoWWlN62EyKAoUmA/edit#gid=2083665562) which contains only entries that I think could be relevant to SatisfIA, ranked by how relevant I think they are. I do not expect the examples towards the end of this list to be very relevant, but I find them very interesting.

The "Requires Learning" column cointains "Yes" if some kind of gradient descent or genetic optimization is needed and the problem cannot be approached with planning.

(Please also see [the SatisfIA project's kanban board](https://github.com/orgs/pik-gane/projects/2) for cards containing links to further environments such as the MACHIAVELLI or EMPA suites of environments. A big challenge for the testing will be to replace the shipped reward function of an environment by a meaningful set of evaluation metrics in terms of which meaningful aspiration-type goals can be specified. E.g., in the Lunar Lander, one could use four evaluation metrics: be the horizontal and vertical positions and velocities of the lander.)  

| Short Description | Requires Learning | Misalignment Occured In The Wild | Long Description | Algorithm | Required Compute | Reference |
| ----------------- | ----------------- | -------------------------------- | ---------------- | --------- | ---------------- | --------- |
| Misgeneralization of correct goals | I think no | No | Agents trained on simple gridwords do not learn the specified goal. | V-MPO | I guess tens of GPU minutes | [ArXiv](https://arxiv.org/abs/2210.01790), sections 3.1 and 3.2 |
| Ant does a cartweel | Yes | Yes | A robotic ant in a simulated environment spins when trained to keep its torso 70cm above the ground. | Not mentioned | I guess tens of minutes on a GPU | [Twitter](https://twitter.com/Karolis_Ram/status/1506607159114670085) |
| CoinRun and similar | Yes | No | This paper trains agents in three simple atari-like game environments with 2D visual observations, test it on a different distribution in the same environment, and observe goal misgeneralization. The environments were made intentionally to study goal misgeneralization. |  PPO but I think one could use DQN as well | I guess hours on one GPU | [ArXiv](https://arxiv.org/abs/2105.14111) |
| CycleGAN steganography | Yes | Yes | A CycleGAN model transforming satellite images to map images and back war trained to make accurate reconstructions. It encoded information steganographically into the map in a way indetectable by humans | CycleGan | I guess hours on one GPU, maybe tens of minutes | [ArXiv](https://arxiv.org/abs/1712.02950) |
| Bayesian optimizer specification gaming | Maybe | Yes | Bayesian optimizer finds unrealistic molecules that are valid according to the computed score | Bayesian optimizer | Not specified | [NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ded98d28f82342a39f371c013dfb3058-Abstract-Conference.html) |
| Agent virbates ball | No | Yes | The agent learned to get to the ball and vibrate touching it as fast as possible | Tabular RL | Not much | [UC Berkeley](http://luthuli.cs.uiuc.edu/~daf/courses/games/AIpapers/ng99policy.pdf) |
| 2D walking robot physics bug | Yes | Yes | Creatures exploited a quirk in Box2D physics by clipping one leg into another to slide along the ground with phantom forces instead of walking | I guess artificial evolution | I guess hours on a CPU | [YouTube](https://youtu.be/K-wIZuAA3EY?t=486) |
| Robots climb over walls | Yes | Yes | Video game robots evolved a "wiggle" to go over walls by exploiting a bug in the physics engine, instead of going around them | Neuroevolution | Trained on CPU(s) in 2005 | [University of Texas](https://nn.cs.utexas.edu/downloads/papers/stanley.ieeetec05.pdf) |
| Boat goes in circle collecting intermediate reward. | Yes | Yes | A boat goes in circles collecting an intermediate reward repeatedly instead of completing the race. | Not mentioned | I guess hours on one GPU | [OpenAI](https://openai.com/research/faulty-reward-functions)
| RLHF robotic hand misleads human | Yes | Yes | The agent tricked a human evaluator by hovering its hand between the camera and the object | RLHF | Less than an hour of human feedback and training | [OpenAI](https://openai.com/research/learning-from-human-preferences) |
| Walking on one leg | Yes | Yes | Walking agent in DMControl suite learns to walk using only one leg | RLHF PPO | At most one hour of human feedback | [ArXiv](https://arxiv.org/abs/2106.05091) |
| Soft robots exploit discrete time physics engine | Yes | Yes | Soft robots penetrate floor between timesteps, which generates repelling energy, giving free energy for locomotion | Artificial Evolution | Not specified | [Jeff Clune's personal website](http://jeffclune.com/publications/2013_Softbots_GECCO.pdf) |
| Robot exploits psysics engine bugs | Yes | Yes | A robotic agent trained in a simulated environment achieves a very high score by exploiting a bug in the psysics simulator. | Artificial evolution | I guess hours or days on one GPU | [ArXiv](https://arxiv.org/abs/1803.03453) |
| RLHF robot bounces back and forth | Yes | Yes | The agent bounces the ball back and forth without scoring | RLHF | At most one hour of human feedback and one GPU day. | [DeepMind](https://deepmind.com/blog/learning-through-human-feedback/) |
| RLHF adversarial on reward model | Yes | Yes | The RL algorithm exploited a goal classifier by moving the robot arm in a peculiar way resulting in an erroneous high reward, since the classifier was not trained on this specific kind of negative example | RLHF | Human feedback + I guess tens of minutes on one GPU | [Berkeley AI Research Lab](https://bair.berkeley.edu/blog/2019/05/28/end-to-end/#problem-with-classifiers) |
| Robotic arm moves table | Yes | Yes | A robotic arm learned to move the table rather than the block. | HER | I guess hours on one GPU | [GitHub issue](https://github.com/openai/gym/issues/920) |
| Robot improvises pocket | Yes | Yes | Four-legged robot learned to drop the ball into a hole in its leg joint and then walk across the floor without the ball falling out | Artificial evolution | CPU hours | [Otoro Blog](https://blog.otoro.net/2017/11/12/evolving-stable-strategies/) |
| Collision detection bug exploit. | Yes | Yes | Creatures exploited a collision detection bug to get free energy by clapping body parts together. Creatures grow really tall and generate high velocities by falling over. | Artificial Evolution | 3 hours on a 32 CPU cluster in 1994 | [Karl Sims's personal website](https://www.karlsims.com/papers/siggraph94.pdf) |
| Bicycle agent cicles | Yes | Yes | Bicicle agent in a physics simulator circles around the goal in a stable loop | SARSA(A) | I guess hours or days on one GPU | [ResearchGate](https://www.researchgate.net/publication/221346431_Learning_to_Drive_a_Bicycle_Using_Reinforcement_Learning_and_Shaping) |
| Model based planner produces bad plans that are good according to the model | Maybe | Yes | RL agents using learned model-based planning paradigms such as model predictive control exploit the learned model by choosing a plan going through the worst-modeled parts of the environment and producing unrealistic plans | Base model + policy gradient | Probably not much | [ArXiv](https://arxiv.org/abs/1703.04070) |
| Atari RLHF misleads humans | Yes | Yes | 1: The agent repeatedly shoots the spider but barely misses it. 2: The agent repeatedly moves towards the key without grabbing it. 3: The agent repeatedly looks left and right. | RLHF with DQN | A few human hours and 200 training hours | [ArXiv](https://arxiv.org/abs/1811.06521) |
| Agent learns to imitate expert instead of doing task. | Yes | No | When trained in presence of an expert, an agent learns to imitate it instead of learning to perform the task. | MPO | I think hours on multilpe GPUs | [ArXiv](https://arxiv.org/abs/2203.00715) |
| Alpha tick tack toe passes forever | Maybe | Yes | A reimplementation of AlphaGo applied to Tic-tac-toe learns to pass forever | I guess minutes on one CPU | [Speaker Deck](https://speakerdeck.com/chewxy/a-funny-thing-happened-on-the-way-to-reimplementing-alphago-in-go?slide=142) |
| Sycophancy | Yes | Yes | Larger language models showed a tendency to express more agreement with the user's stated views. This happens both for pretrained models and models fine-tuned with human feedback (RLHF). | Base model or RLHFed LLM | API call | [ArXiv](https://arxiv.org/abs/2212.09251) |
| Classifiers relying on spurious correlations | Yes | Yes | Lots of classifiers rely on spurious correlations valid during training which sometimes (often?) misgeneralize and/or lead to bias. | Supervised learning | Depends | A big fraction of the examples [here](https://docs.google.com/spreadsheets/d/1uXzWavy1mS0X-uQ21UPWHlAHjXFJoWWlN62EyKAoUmA/edit#gid=2083665562) |
| Block moving robot games specification | Yes | Yes | When trained to place a red block on top of a blue block, the agent flips the red block rather than lifting it and placing on top of the blue block. | DPG-R | At most one GPU hour | [ArXiv](https://arxiv.org/abs/1704.03073) |
| Summarization model games metric | Yes | Yes | An effort at a ROUGE-only summarization NN produced largely gibberish summaries because it gamed the ROGUE metric used to evaluate summarizations. | Self critical policy gradient | Not specified | [ArXiv](https://arxiv.org/pdf/1705.04304.pdf) |
| Football player kicks ball out of bounds | Yes | Yes | Rather than shooting at the goal, the player kicks the ball out of bounds. Someone from the other team has to throw the ball in (in this case the goalie), so now the player has a clear shot at the goal. | IMPALA, PPO, Ape-X DQN | Hours on one GPU | [ArXiv](https://arxiv.org/abs/1907.11180) |
| Mujoco overflow error exploit | Yes | Yes | Model-based RL algorithm exploits an overflow error in a mujoco environment to achieve high speed by spinning | Some model based RL algorithm | I guess tens of GPU minutes | [ArXiv](https://arxiv.org/abs/2102.13651) |
| Hide and seek robots exploit physics engine | Yes | Yes | Box surfing: Since agents move by applying forces to themselves, they can grab a box while on top of it and “surf” it to the hider’s location. Endless running: Without adding explicit negative rewards for agents leaving the play area, in rare cases hiders will learn to take a box and endlessly run with it. Ramp exploitation (hiders): Hiders abuse the contact physics and remove ramps from the play area. Ramp exploitation (seekers): Seekers learn that if they run at a wall with a ramp at the right angle, they can launch themselves upward. | PPO | GPU days | [OpenAI](https://openai.com/research/emergent-tool-use#surprisingbehaviors) |
| Mario Suicide | Kind of | Yes | Mario commits suicide to teleport to spawn instead of travelling. | Hardcoded search inspecting the game's internal memory |  Hundreds of CPU hours in 2013 | [Carnegie Mellon University](https://www.cs.cmu.edu/~tom7/mario/mario.pdf) |
| Atari agent gets reward by killing the first enemy over and over | Yes | Yes | The agent learns to stay on the first floor and kill the first enemy over and over to get a small amount of reward | Rainbow-IQN | One GPU week | [ArXiv](https://arxiv.org/abs/1908.04683) |
| Intentionally crashing game | Yes | Yes | Since the AIs were more likely to get 'killed' if they lost a game, being able to crash the game was an advantage for the genetic selection process. | Genetic Algorithm | One CPU day in 2008 | [Pomona College](https://cs.pomona.edu/~mwu/CourseWebpages/CS190-fall15-Webpage/Readings/2008-Gameplaying.pdf) |
| Atari agent infinite reweard | Yes | Yes | 1: Agent learns to bait an opponent into following it off a cliff, which gives it enough points for an extra life, which it does forever in an infinite loop. 2: The agent discovers an in-game bug. For a reason unknown to us, the game does not advance to the second round but the platforms start to blink and the agent quickly gains a huge amount of points (close to 1 million for our episode time limit) | Evolution Strategy | 10 hours on a cluster of 400 CPUs | [ArXiv](https://arxiv.org/abs/1802.08842) |
| Atari bug exploit | Yes | Yes | The agent learns to exploit a flaw in the emulator to make a key re-appear. Note that this may be an intentional feature of the game rather than a bug, as discussed [here](https://news.ycombinator.com/item?id=17460392) | PPO | I guess hours on 8 GPUs | [OpenAI](https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration) |
| Robot runs weirdly | Yes | Yes | A simulated musculoskeletal model learns to run by learning unusual gaits (hopping, pigeon jumps, diving) to increase its reward | PPO | GPU days | [ArXiv](https://arxiv.org/abs/1804.00361) |
| Collecting infinite reward in game instead of moving to next level | Yes | Yes | Go Explore agent learns to perform a specific sequence of actions, which allow it to exploit a bug and remain in the treasure room (the final room before being sent to the next level) indefinitely and collect unlimited points, instead of being automatically moved to the next level | Go-Explore | 10 hours on high end CPU machine | [ArXiv](https://arxiv.org/abs/1901.10995) |
| R2D3 bug exploit | Yes | Yes | R2D3 agent exploited a bug by tricking the sensor into remaining active even when it is not in contact with the key, by pressing the key against it in a precise way | R2D3 | Not specified | [ArXiv](https://arxiv.org/abs/1909.01387) |
| LLM convergent instrumental goals | Yes | Yes | Larger LMs and those fine-tuned with RLHF "more often give answers that indicate a willingness to pursue potentially dangerous subgoals: resource acquisition, optionality preservation, goal preservation, power seeking, and more. | RLHF | API call | [ArXiv](https://arxiv.org/abs/2212.09251) |
| InstructGPT follows instructions even when those are harmful | Yes | Yes | When instruction tuned to give helpful, honest, and harmless answers, GPT-3 learns to just do what it is told. | RLHF | Fine tune API call or hours on a GPU. | [OpenAI's website](https://openai.com/research/instruction-following) |
| Atari suicide | Yes | Yes | Agent kills itself at the end of level 1 to avoid losing in level 2. | Human Intervention RL | Not specified | [Owain Evans's blog](https://owainevans.github.io/blog/hirl_blog.html) |
| Automated program repair algorithm generated stupid program which passed all tests | Maybe | Yes | Genetic debugging algorithm GenProg made the program output an empty list, which was considered a sorted list by the evaluation metric. Genetic debugging algorithm GenProg learned to delete the target output file and get the program to output nothing. | Genetic algorithm | I guess CPU minutes or hours | [University of Michigan](https://web.eecs.umich.edu/~weimerw/p/weimer-ssbse2013.pdf) |
| Fall from long legs instead of walking | Yes | Yes | An agent that could modify its own body learned to have extremely long legs that allowed it to fall forward and reach the goal without walking | Evolution for body shape and then simple policy gradient | Days on a CPU cluster | [DesignRL](https://designrl.github.io/) |
| Species: Artificial Life bug exploits | Yes | Yes | Evolved creatures in the Species: Artificial life computer game found a survival strategy where they could "gain energy by suffocating themselves", and "breed multiple times on a single frame, or while paused, without paying the energy cost" due to a bug. | Artificial evolution | At most hours on a CPU | [SpeciesDevBlog](https://speciesdevblog.wordpress.com/2018/10/04/0-11-0-910-all-the-good-things/) |
| Science LLM hallucinated papers | Yes | Yes | Galactica language model made up fake papers (sometimes attributing them to real authors) | LLM | API call | [TechnologyReview](https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/) |
| Species exploits evolution engine | Yes | Yes | A species in an artificial life simulation evolved a sedentary lifestyle that consisted mostly of mating in order to produce new children which could be eaten (or used as mates to produce more edible children) | Artificial evolution | The paper is from 1994 | [ResearchGate](https://www.researchgate.net/profile/Larry-Yaeger/publication/2448680_Computational_Genetics_Physiology_Metabolism_Neural_Systems_Learning_Vision_and_Behavior_or_PolyWorld_Life_in_a_New_Context/links/0912f50e101b77ec67000000/Computational-Genetics-Physiology-Metabolism-Neural-Systems-Learning-Vision-and-Behavior-or-PolyWorld-Life-in-a-New-Context.pdf) |
| Lego robot drives wrong | Yes | Yes | A robot with with three actions (go forward, turn left, turn right) learned to reverse along a straight section of a path by alternating left and right turns | Tabular Q learning | You need a physical lego robot | [ReseaerchGate](https://www.researchgate.net/publication/228953260_Lego_Mindstorms_Robots_as_a_Platform_for_Teaching_Reinforcement_Learning/link/09e415100c199e8c9b000000/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19) |
| Circuit design with disconnected logic gate | Yes | Yes | A genetic algorithm designed a circuit with a disconnected logic gate that was necessary for it to function (exploiting peculiarities of the hardware) | Genetic algorithm | FPGA hardware needed | [ResearchGate](https://www.researchgate.net/publication/2737441_An_Evolved_Circuit_Intrinsic_in_Silicon_Entwined_With_Physics/link/56f1ffc908ae4744a91eff8d/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19) |

## Bonus

Those are things about the cases I found very interesting despite them being irrelevant for SatisfIA.

The cases I find most interesting were:
- In 1997, researchers ran a genetic algorithm to generate logic circuits on an FPGA (piece of hardware that efficiently emulates arbitrary logic circuits). Most of half of one evolved circuit was not connected to the input or the output - so it should not affect the behavior of this circuit. However, when the researchers deleted this seemingly useless part, the circuit stopped working! It was conjectured that the seemingly useless part was interacting with the rest of the circuit through the power-supply wiring or electromagnetic coupling. [Reference](https://www.researchgate.net/publication/2737441_An_Evolved_Circuit_Intrinsic_in_Silicon_Entwined_With_Physics/link/56f1ffc908ae4744a91eff8d/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19)
- CycleGAN was trained to transform satellite photographs into maps and back. However, it was able to reconstruct details that one should not be able to reconstruct from a map suspiciously well. It then turns out CycleGAN was encoding information steganographically into the map. [Reference](https://arxiv.org/abs/1712.02950)

The funniest one is:
- Agent kills itself at the end of level 1 to avoid losing in level 2. [Reference](https://owainevans.github.io/blog/hirl_blog.html)

This one could be used as a plot for a horror movie:
- A species in an artificial life simulation evolved a sedentary lifestyle that consisted mostly of mating in order to produce new children which could be eaten (or used as mates to produce more edible children). [Reference](https://www.researchgate.net/profile/Larry-Yaeger/publication/2448680_Computational_Genetics_Physiology_Metabolism_Neural_Systems_Learning_Vision_and_Behavior_or_PolyWorld_Life_in_a_New_Context/links/0912f50e101b77ec67000000/Computational-Genetics-Physiology-Metabolism-Neural-Systems-Learning-Vision-and-Behavior-or-PolyWorld-Life-in-a-New-Context.pdf)
